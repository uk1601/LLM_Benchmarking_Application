{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def dbconnect(**params):\n",
    "    return psycopg2.connect(**params)\n",
    "\n",
    "params = {\n",
    "    'host': \"******\",\n",
    "    'port': \"******\",\n",
    "    'database': \"******\",\n",
    "    'user': \"******\",\n",
    "    'password': \"******\"\n",
    "}\n",
    "\n",
    "conn = dbconnect(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur=conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "llmresponses\n",
      "llms\n",
      "tasks\n"
     ]
    }
   ],
   "source": [
    "def view_tables(cur):\n",
    "    # Query to list all tables in the database\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'public'\n",
    "        ORDER BY table_name;\n",
    "    \"\"\")\n",
    "\n",
    "    # Fetch and print all the table names\n",
    "    tables = cur.fetchall()\n",
    "    print(\"Tables in the database:\")\n",
    "    for table in tables:\n",
    "        print(table[0])\n",
    "\n",
    "view_tables(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table:tasks\n",
      "Columns: ['taskid', 'question', 'expectedanswer', 'level', 'filename', 'filepath', 'annotations']\n",
      "Table:llms\n",
      "Columns: ['llmid', 'llmname', 'version', 'parameters']\n",
      "Table:llmresponses\n",
      "Columns: ['responseid', 'taskid', 'llmid', 'responsetext', 'isannotated', 'resultcategory', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "def view_table_data(cur,table_name,data=False):\n",
    "    # Replace 'your_table_name' with the name of the table you want to retrieve data from\n",
    "    # Change this to the table you want to query\n",
    "\n",
    "    # Execute a query to retrieve all rows from the table\n",
    "    query = f\"SELECT * FROM {table_name};\"\n",
    "    cur.execute(query)\n",
    "\n",
    "    # Fetch all rows from the table\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    # Print the column names (optional)\n",
    "    colnames = [desc[0] for desc in cur.description]\n",
    "    print(f\"Table:{table_name}\\nColumns: {colnames}\")\n",
    "\n",
    "    if data:\n",
    "        # Print each row\n",
    "        print(f\"Data from table '{table_name}':\")\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "\n",
    "view_table_data(cur,\"tasks\")\n",
    "view_table_data(cur,\"llms\")\n",
    "view_table_data(cur,\"llmresponses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table:llms\n",
      "Columns: ['llmid', 'llmname', 'version', 'parameters']\n",
      "Data from table 'llms':\n",
      "Table:llmresponses\n",
      "Columns: ['responseid', 'taskid', 'llmid', 'responsetext', 'isannotated', 'resultcategory', 'timestamp']\n",
      "Data from table 'llmresponses':\n"
     ]
    }
   ],
   "source": [
    "view_table_data(cur,\"llms\",True)\n",
    "view_table_data(cur,\"llmresponses\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted data into llms table.\n"
     ]
    }
   ],
   "source": [
    "# DATA ALREADY INSERTED\n",
    "\n",
    "# # Insert test data into 'llms' table\n",
    "# llms_data = [\n",
    "#     (1, 'GPT-3', 'v1', 175000000000),\n",
    "#     (2, 'GPT-4', 'v2', 175000000000),\n",
    "#     (3, 'ChatGPT', 'v1', 6800000000)\n",
    "# ]\n",
    "\n",
    "# cur.executemany(\n",
    "#     \"INSERT INTO llms (llmid, llmname, version, parameters) VALUES (%s, %s, %s, %s) ON CONFLICT DO NOTHING\",\n",
    "#     llms_data\n",
    "# )\n",
    "\n",
    "# conn.commit()\n",
    "# print(\"Inserted data into llms table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted data into llmresponses table.\n"
     ]
    }
   ],
   "source": [
    "#DATA ALREADY INTO LLMRESPONSES\n",
    "\n",
    "# # Fetch all taskids from the 'tasks' table\n",
    "# cur.execute(\"SELECT taskid, expectedanswer FROM tasks\")\n",
    "# tasks = cur.fetchall()\n",
    "\n",
    "# # Fetch all llmids from the 'llms' table\n",
    "# cur.execute(\"SELECT llmid FROM llms\")\n",
    "# llms = cur.fetchall()\n",
    "\n",
    "# # Define possible values for 'isannotated' and 'resultcategory'\n",
    "# is_annotated_options = ['yes', 'no']\n",
    "# result_category_options = ['AS IS', 'With Annotation', 'Helpless!']\n",
    "\n",
    "# # Populate 'llmresponses' table with random data\n",
    "# for i in range(101, 501):  # Insert 10 rows of test data\n",
    "#     task = random.choice(tasks)  # Randomly select a task\n",
    "#     llm = random.choice(llms)  # Randomly select an LLM\n",
    "\n",
    "#     taskid = task[0]  # taskid from tasks table\n",
    "#     llmid = llm[0]  # llmid from llms table\n",
    "#     responsetext = task[1]  # Use expectedanswer from tasks as responsetext\n",
    "#     isannotated = random.choice(is_annotated_options)  # Randomly select yes or no\n",
    "#     resultcategory = random.choice(result_category_options)  # Randomly select category\n",
    "#     timestamp = datetime.now()  # Current timestamp\n",
    "\n",
    "#     cur.execute(\"\"\"\n",
    "#         INSERT INTO llmresponses (responseid, taskid, llmid, responsetext, isannotated, resultcategory, timestamp)\n",
    "#         VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "#     \"\"\", (i, taskid, llmid, responsetext, isannotated, resultcategory, timestamp))\n",
    "\n",
    "# conn.commit()\n",
    "# print(\"Inserted data into llmresponses table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'view_table_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mview_table_data\u001b[49m(cur,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m view_table_data(cur,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllmresponses\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'view_table_data' is not defined"
     ]
    }
   ],
   "source": [
    "view_table_data(cur,\"llms\",True)\n",
    "view_table_data(cur,\"llmresponses\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_cal(query):\n",
    "    cur.execute(query)\n",
    "    results=cur.fetchall()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Metrics <br>\n",
    "### Overall Accuracy per LLM (As Is):<br>\n",
    "Formula: (Number of \"As Is\" responses without annotation) / (Total number of tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ID: 1, Overall Accuracy (As Is): 0.00000000000000000000000000000000000000000000000000%\n",
      "LLM ID: 2, Overall Accuracy (As Is): 0.00000000000000000000000000000000000000000000000000%\n",
      "LLM ID: 3, Overall Accuracy (As Is): 0.00000000000000000000000000000000000000000000000000%\n"
     ]
    }
   ],
   "source": [
    "sql=''' SELECT\n",
    "    LLMId,\n",
    "    COUNT(CASE WHEN IsAnnotated = FALSE AND ResultCategory = 'As Is' THEN 1 END) * 100.0 / COUNT(DISTINCT TaskId) AS AccuracyPercentage\n",
    "FROM\n",
    "    LLMResponses\n",
    "GROUP BY\n",
    "    LLMId;'''\n",
    "\n",
    "acc_per_llm= metric_cal(sql)\n",
    "for row in acc_per_llm:\n",
    "    print(f\"LLM ID: {row[0]}, Overall Accuracy (As Is): {row[1]:.50f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy with Annotation:**\n",
    "\n",
    "- **Formula:** (Number of \"Correct with annotation\" responses) / (Number of tasks that needed annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ID: 1, Accuracy with Annotation: 31.11%\n",
      "LLM ID: 3, Accuracy with Annotation: 41.11%\n",
      "LLM ID: 2, Accuracy with Annotation: 29.17%\n"
     ]
    }
   ],
   "source": [
    "# Query to calculate accuracy with annotation\n",
    "query = \"\"\"\n",
    "    SELECT llmid,\n",
    "           COUNT(CASE WHEN isannotated = True AND resultcategory = 'With Annotation' THEN 1 END) * 100.0 /\n",
    "           COUNT(CASE WHEN isannotated = True THEN 1 END) AS accuracy_with_annotation\n",
    "    FROM llmresponses\n",
    "    GROUP BY llmid;\n",
    "\"\"\"\n",
    "acc_with_annotation=metric_cal(query)\n",
    "# Display results\n",
    "for row in acc_with_annotation:\n",
    "    print(f\"LLM ID: {row[0]}, Accuracy with Annotation: {row[1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement Rate\n",
    "\n",
    "- **Formula:** (Number of tasks corrected after annotation) / (Number of tasks initially incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ID: 1, Improvement Rate: 53.85%\n",
      "LLM ID: 3, Improvement Rate: 72.55%\n",
      "LLM ID: 2, Improvement Rate: 38.18%\n"
     ]
    }
   ],
   "source": [
    "# Query to calculate improvement rate\n",
    "query = \"\"\"\n",
    "    SELECT llmid,\n",
    "           COUNT(CASE WHEN isannotated = 'yes' AND resultcategory = 'With Annotation' THEN 1 END) * 100.0 /\n",
    "           COUNT(CASE WHEN isannotated = 'no' AND resultcategory != 'AS IS' THEN 1 END) AS improvement_rate\n",
    "    FROM llmresponses\n",
    "    GROUP BY llmid;\n",
    "\"\"\"\n",
    "impv_rate=metric_cal(query)\n",
    "\n",
    "# Display results\n",
    "for row in impv_rate:\n",
    "    print(f\"LLM ID: {row[0]}, Improvement Rate: {row[1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Failure Rate after Annotation**\n",
    "\n",
    "- **Formula:** (Number of \"Failed even after annotation\" responses) / (Total number of tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ID: 1, Failure Rate after Annotation: 30.36%\n",
      "LLM ID: 2, Failure Rate after Annotation: 29.17%\n",
      "LLM ID: 3, Failure Rate after Annotation: 24.30%\n"
     ]
    }
   ],
   "source": [
    "# Query to calculate failure rate after annotation\n",
    "query = \"\"\"\n",
    "    SELECT llmid,\n",
    "           COUNT(CASE WHEN isannotated = 'yes' AND resultcategory = 'Helpless!' THEN 1 END) * 100.0 /\n",
    "           COUNT(DISTINCT taskid) AS failure_rate_after_annotation\n",
    "    FROM llmresponses\n",
    "    GROUP BY llmid;\n",
    "\"\"\"\n",
    "fail_rate_annotation=metric_cal(query)\n",
    "\n",
    "# Display results\n",
    "for row in fail_rate_annotation:\n",
    "    print(f\"LLM ID: {row[0]}, Failure Rate after Annotation: {row[1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance by Task Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ID: 1, Task Level: 1, Overall Accuracy (As Is): 23.33%\n",
      "LLM ID: 1, Task Level: 2, Overall Accuracy (As Is): 29.51%\n",
      "LLM ID: 1, Task Level: 3, Overall Accuracy (As Is): 19.05%\n",
      "LLM ID: 2, Task Level: 1, Overall Accuracy (As Is): 38.46%\n",
      "LLM ID: 2, Task Level: 2, Overall Accuracy (As Is): 34.69%\n",
      "LLM ID: 2, Task Level: 3, Overall Accuracy (As Is): 19.05%\n",
      "LLM ID: 3, Task Level: 1, Overall Accuracy (As Is): 34.29%\n",
      "LLM ID: 3, Task Level: 2, Overall Accuracy (As Is): 25.00%\n",
      "LLM ID: 3, Task Level: 3, Overall Accuracy (As Is): 25.00%\n"
     ]
    }
   ],
   "source": [
    "# Query to calculate overall accuracy per LLM and task level\n",
    "query = \"\"\"\n",
    "    SELECT llr.llmid, t.level,\n",
    "           COUNT(CASE WHEN llr.isannotated = 'no' AND llr.resultcategory = 'AS IS' THEN 1 END) * 100.0 / \n",
    "           COUNT(DISTINCT llr.taskid) AS accuracy_percentage\n",
    "    FROM llmresponses llr\n",
    "    JOIN tasks t ON llr.taskid = t.taskid\n",
    "    GROUP BY llr.llmid, t.level;\n",
    "\"\"\"\n",
    "Perf_task=metric_cal(query)\n",
    "# Display results\n",
    "for row in Perf_task:\n",
    "    print(f\"LLM ID: {row[0]}, Task Level: {row[1]}, Overall Accuracy (As Is): {row[2]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-1-gaia-jiakDA-V-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
